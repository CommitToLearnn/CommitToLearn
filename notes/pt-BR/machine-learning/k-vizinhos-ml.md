# K-Vizinhos Mais Pr√≥ximos (KNN) em Machine Learning

O **K-Nearest Neighbors (KNN)** √© um dos algoritmos de machine learning mais fundamentais e intuitivos. Diferente de outros algoritmos que constroem modelos complexos durante o treinamento, o KNN √© um algoritmo "pregui√ßoso" que simplesmente armazena os dados e faz predi√ß√µes baseadas na similaridade.

## Por que KNN √© Machine Learning?

### Aprendizado Supervisionado
- **Aprende com exemplos rotulados** durante a fase de treinamento
- **Generaliza para novos dados** n√£o vistos anteriormente
- **Melhora com mais dados** de qualidade

### Capacidade de Generaliza√ß√£o
- **Identifica padr√µes** nos dados de treinamento
- **Aplica conhecimento** para classificar/prever novos casos
- **Adapta-se** a diferentes tipos de problemas

## KNN vs Outros Algoritmos de ML

| Caracter√≠stica | KNN | √Årvore de Decis√£o | SVM | Redes Neurais |
|---|---|---|---|---|
| **Tipo** | Instance-based | Model-based | Model-based | Model-based |
| **Treinamento** | Instant√¢neo | M√©dio | Lento | Muito lento |
| **Predi√ß√£o** | Lento | R√°pido | R√°pido | R√°pido |
| **Interpretabilidade** | Alta | Alta | Baixa | Muito baixa |
| **Mem√≥ria** | Alta | Baixa | M√©dia | M√©dia |

## Aplica√ß√µes em Machine Learning

### **Sistemas de Recomenda√ß√£o**
```python
# Recomendar filmes baseado em usu√°rios similares
def recomendar_filmes(usuario_id, k=5):
    # Encontrar K usu√°rios mais similares
    usuarios_similares = knn.kneighbors([perfil_usuario], n_neighbors=k)
    
    # Recomendar filmes que esses usu√°rios gostaram
    filmes_recomendados = []
    for usuario_similar in usuarios_similares:
        filmes_recomendados.extend(filmes_curtidos[usuario_similar])
    
    return filmes_recomendados
```

### **Detec√ß√£o de Anomalias**
```python
# Detectar transa√ß√µes fraudulentas
def detectar_fraude(transacao, k=10):
    # Encontrar K transa√ß√µes mais similares
    vizinhos = knn.kneighbors([transacao], n_neighbors=k)
    
    # Se a dist√¢ncia m√©dia for muito alta, pode ser anomalia
    distancia_media = np.mean(vizinhos.distances)
    
    return distancia_media > threshold_anomalia
```

### **Classifica√ß√£o de Imagens**
```python
# Classificar d√≠gitos manuscritos
from sklearn.datasets import load_digits
from sklearn.neighbors import KNeighborsClassifier

digits = load_digits()
knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(digits.data, digits.target)

# Predizer novo d√≠gito
novo_digito = digits.data[0].reshape(1, -1)
predicao = knn.predict(novo_digito)
```

### **Processamento de Linguagem Natural**
```python
# Classifica√ß√£o de sentimentos em textos
from sklearn.feature_extraction.text import TfidfVectorizer

# Vetorizar textos
vectorizer = TfidfVectorizer(max_features=1000)
X_vectorized = vectorizer.fit_transform(textos)

# Treinar KNN
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_vectorized, sentimentos)
```

## KNN na Pipeline de Machine Learning

### **Pr√©-processamento Cr√≠tico**
```python
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

# Pipeline completa para KNN
pipeline_knn = Pipeline([
    ('scaler', StandardScaler()),  # ESSENCIAL para KNN
    ('knn', KNeighborsClassifier(n_neighbors=5))
])

pipeline_knn.fit(X_train, y_train)
```

### **Valida√ß√£o e Tuning**
```python
from sklearn.model_selection import GridSearchCV

# Busca pelos melhores hiperpar√¢metros
param_grid = {
    'n_neighbors': [3, 5, 7, 9, 11],
    'weights': ['uniform', 'distance'],
    'metric': ['euclidean', 'manhattan']
}

grid_search = GridSearchCV(
    KNeighborsClassifier(),
    param_grid,
    cv=5,
    scoring='accuracy'
)

grid_search.fit(X_train, y_train)
print(f"Melhores par√¢metros: {grid_search.best_params_}")
```

## Vantagens do KNN em ML

### **Simplicidade**
- F√°cil de entender e implementar
- N√£o requer conhecimento de matem√°tica avan√ßada
- √ìtimo para prototipagem r√°pida

### **Flexibilidade**
- Funciona com dados num√©ricos e categ√≥ricos
- Pode ser usado para classifica√ß√£o e regress√£o
- Adapta-se a qualquer distribui√ß√£o de dados

### **Interpretabilidade**
- Decis√µes s√£o explic√°veis
- Pode-se ver exatamente quais exemplos influenciaram a predi√ß√£o
- √ötil em dom√≠nios onde explicabilidade √© crucial

## Limita√ß√µes em Contextos de ML

### **Escalabilidade**
```python
# Problema: O(n) para cada predi√ß√£o
import time

start_time = time.time()
predictions = knn.predict(X_test)  # Lento para datasets grandes
end_time = time.time()

print(f"Tempo de predi√ß√£o: {end_time - start_time:.2f} segundos")
```

### **Maldi√ß√£o da Dimensionalidade**
```python
# Demonstra√ß√£o da degrada√ß√£o com muitas dimens√µes
from sklearn.datasets import make_classification

# Performance com poucas dimens√µes
X_low, y = make_classification(n_features=5, n_informative=5)
score_low = cross_val_score(knn, X_low, y, cv=5).mean()

# Performance com muitas dimens√µes
X_high, y = make_classification(n_features=100, n_informative=5)
score_high = cross_val_score(knn, X_high, y, cv=5).mean()

print(f"Score (5 features): {score_low:.3f}")
print(f"Score (100 features): {score_high:.3f}")
```

### **Sensibilidade a Dados Desbalanceados**
```python
from imblearn.over_sampling import SMOTE

# Lidar com dados desbalanceados
smote = SMOTE(random_state=42)
X_balanced, y_balanced = smote.fit_resample(X_train, y_train)

knn.fit(X_balanced, y_balanced)
```

## Otimiza√ß√µes para ML em Produ√ß√£o

### **Estruturas de Dados Eficientes**
```python
from sklearn.neighbors import NearestNeighbors
from sklearn.neighbors import BallTree, KDTree

# Para baixas dimens√µes (< 20)
kd_tree = KDTree(X_train)

# Para altas dimens√µes
ball_tree = BallTree(X_train)

# Para datasets muito grandes
from sklearn.neighbors import LSHForest  # Approximate
```

### **Redu√ß√£o de Dimensionalidade**
```python
from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectKBest

# Reduzir dimens√µes antes do KNN
pca = PCA(n_components=50)
X_reduced = pca.fit_transform(X_train)

knn.fit(X_reduced, y_train)
```

### **Sampling Inteligente**
```python
from imblearn.under_sampling import EditedNearestNeighbours

# Remover amostras redundantes ou ruidosas
enn = EditedNearestNeighbours()
X_cleaned, y_cleaned = enn.fit_resample(X_train, y_train)
```

## Compara√ß√£o com Deep Learning

| Aspecto | KNN | Deep Learning |
|---|---|---|
| **Dados necess√°rios** | Poucos | Muitos |
| **Tempo de treinamento** | Instant√¢neo | Longo |
| **Interpretabilidade** | Alta | Baixa |
| **Feature engineering** | Necess√°ria | Autom√°tica |
| **Overfitting** | Control√°vel por K | Complexo |

## KNN em Ensemble Methods

```python
from sklearn.ensemble import VotingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC

# Combinar KNN com outros algoritmos
ensemble = VotingClassifier([
    ('knn', KNeighborsClassifier(n_neighbors=5)),
    ('tree', DecisionTreeClassifier()),
    ('svm', SVC(probability=True))
], voting='soft')

ensemble.fit(X_train, y_train)
```

## Quando Usar KNN em ML

### ‚úÖ **Use KNN quando:**
- Dataset pequeno/m√©dio (< 100k amostras)
- Poucas dimens√µes (< 50 features)
- Precisar de interpretabilidade
- Dados localmente estruturados
- Prototipagem r√°pida

### ‚ùå **Evite KNN quando:**
- Dataset muito grande
- Muitas dimens√µes
- Dados muito ruidosos
- Precisar de predi√ß√µes em tempo real
- Dados esparsos

## Implementa√ß√£o Completa para ML

```python
class KNNClassifierML:
    def __init__(self, k=5, weight_func='uniform'):
        self.k = k
        self.weight_func = weight_func
        self.is_fitted = False
    
    def fit(self, X, y):
        """Treinar o modelo (apenas armazenar dados)"""
        self.X_train = np.array(X)
        self.y_train = np.array(y)
        self.classes = np.unique(y)
        self.is_fitted = True
        return self
    
    def predict_proba(self, X):
        """Retornar probabilidades de classe"""
        if not self.is_fitted:
            raise ValueError("Modelo n√£o foi treinado")
        
        probabilities = []
        for x in X:
            # Encontrar K vizinhos
            distances = np.sqrt(np.sum((self.X_train - x)**2, axis=1))
            k_indices = np.argsort(distances)[:self.k]
            
            # Calcular pesos
            if self.weight_func == 'distance':
                weights = 1 / (distances[k_indices] + 1e-8)
            else:
                weights = np.ones(self.k)
            
            # Calcular probabilidades
            class_probs = np.zeros(len(self.classes))
            for i, class_label in enumerate(self.classes):
                mask = self.y_train[k_indices] == class_label
                class_probs[i] = np.sum(weights[mask])
            
            # Normalizar
            class_probs /= np.sum(class_probs)
            probabilities.append(class_probs)
        
        return np.array(probabilities)
    
    def predict(self, X):
        """Fazer predi√ß√µes"""
        probas = self.predict_proba(X)
        return self.classes[np.argmax(probas, axis=1)]
```

## Conclus√£o

O KNN √© um algoritmo fundamental em machine learning que oferece:

- **Simplicidade** para come√ßar
- **Interpretabilidade** para explicar decis√µes
- **Flexibilidade** para diferentes problemas
- **Base te√≥rica** para entender conceitos de ML

Embora tenha limita√ß√µes de escala, continua sendo uma ferramenta valiosa no toolkit de qualquer cientista de dados, especialmente para:
- **Baseline models**
- **Sistemas de recomenda√ß√£o**
- **Detec√ß√£o de anomalias**
- **Problemas com poucos dados**

üîó **Para detalhes t√©cnicos completos:** [K-Vizinhos (Algoritmos)](../algoritmos/k-vizinhos.md)

üîó **Para conceitos de overfitting/underfitting:** [Overfitting e Underfitting](overfitting-underfitting.md)
