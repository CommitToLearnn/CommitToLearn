# K-Vizinhos Mais Pr√≥ximos (K-Nearest Neighbors - KNN)

O algoritmo K-Nearest Neighbors (KNN) √© um dos algoritmos de aprendizado de m√°quina mais simples e intuitivos. Ele classifica novos pontos de dados baseado na "vizinhan√ßa" - ou seja, nos K pontos mais pr√≥ximos no conjunto de treinamento.

## Como Funciona?

- **Escolha o valor de K** (n√∫mero de vizinhos)
- **Calcule a dist√¢ncia** entre o novo ponto e todos os pontos do conjunto de treinamento
- **Encontre os K vizinhos mais pr√≥ximos**
- **Para classifica√ß√£o:** Vote pela classe mais comum entre os K vizinhos
- **Para regress√£o:** Calcule a m√©dia dos valores dos K vizinhos

## Caracter√≠sticas Principais

- **Algoritmo "pregui√ßoso" (Lazy):** N√£o constr√≥i um modelo durante o treinamento
- **N√£o-param√©trico:** N√£o faz suposi√ß√µes sobre a distribui√ß√£o dos dados
- **Baseado em inst√¢ncias:** Armazena todo o conjunto de treinamento
- **Sens√≠vel √† escala:** Dist√¢ncias podem ser dominadas por features com valores maiores

## M√©tricas de Dist√¢ncia

### Dist√¢ncia Euclidiana (mais comum)
```
d = ‚àö[(x‚ÇÅ-x‚ÇÇ)¬≤ + (y‚ÇÅ-y‚ÇÇ)¬≤]
```

### Dist√¢ncia de Manhattan
```
d = |x‚ÇÅ-x‚ÇÇ| + |y‚ÇÅ-y‚ÇÇ|
```

### Dist√¢ncia de Minkowski
```
d = (Œ£|x·µ¢-y·µ¢|·µñ)^(1/p)
```

## Exemplo Pr√°tico: Classifica√ß√£o de Filmes

Imagine que queremos classificar se um filme √© "Com√©dia" ou "Drama" baseado em duas caracter√≠sticas:
- Dura√ß√£o (minutos)
- Classifica√ß√£o et√°ria

**Dados de treinamento:**
```
Filme A: (90 min, 12 anos) ‚Üí Com√©dia
Filme B: (150 min, 16 anos) ‚Üí Drama  
Filme C: (95 min, 10 anos) ‚Üí Com√©dia
Filme D: (140 min, 18 anos) ‚Üí Drama
Filme E: (85 min, 14 anos) ‚Üí Com√©dia
```

**Novo filme:** (100 min, 12 anos) ‚Üí ?

**Com K=3:**
- Calcular dist√¢ncias para todos os filmes
- Encontrar os 3 mais pr√≥ximos
- Votar pela classe majorit√°ria

## Escolhendo o Valor de K

### K muito pequeno (K=1):
- **Vantagem:** Sens√≠vel a padr√µes locais
- **Desvantagem:** Sens√≠vel a ru√≠do e outliers

### K muito grande:
- **Vantagem:** Mais est√°vel, menos sens√≠vel a ru√≠do
- **Desvantagem:** Pode ignorar padr√µes locais

### Regras pr√°ticas:
- **K √≠mpar** para evitar empates em classifica√ß√£o bin√°ria
- **K = ‚àön** onde n √© o n√∫mero de amostras de treinamento
- **Valida√ß√£o cruzada** para encontrar o K √≥timo

## Implementa√ß√£o Simples (Python)

```python
import numpy as np
from collections import Counter

class KNN:
    def __init__(self, k=3):
        self.k = k
    
    def fit(self, X, y):
        self.X_train = X
        self.y_train = y
    
    def euclidean_distance(self, x1, x2):
        return np.sqrt(np.sum((x1 - x2)**2))
    
    def predict(self, X):
        predictions = []
        for x in X:
            # Calcular dist√¢ncias
            distances = [self.euclidean_distance(x, x_train) 
                        for x_train in self.X_train]
            
            # Encontrar K vizinhos mais pr√≥ximos
            k_indices = np.argsort(distances)[:self.k]
            k_nearest_labels = [self.y_train[i] for i in k_indices]
            
            # Votar pela classe mais comum
            most_common = Counter(k_nearest_labels).most_common(1)
            predictions.append(most_common[0][0])
        
        return predictions
```

## Vantagens

- **Simplicidade:** F√°cil de entender e implementar
- **N√£o faz suposi√ß√µes:** Funciona com dados n√£o-lineares
- **Versatilidade:** Serve para classifica√ß√£o e regress√£o
- **Adaptabilidade:** Pode se adaptar facilmente a novos dados

## Desvantagens

- **Computacionalmente caro:** O(n) para cada predi√ß√£o
- **Sens√≠vel √† escala:** Features com escalas diferentes podem dominar
- **Sens√≠vel √† dimensionalidade:** Performance degrada em altas dimens√µes (maldi√ß√£o da dimensionalidade)
- **Mem√≥ria:** Precisa armazenar todo o conjunto de treinamento

## Melhorias e Varia√ß√µes

### KNN Ponderado
- Dar mais peso aos vizinhos mais pr√≥ximos
- Peso = 1/dist√¢ncia

### Normaliza√ß√£o de Features
```python
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_normalized = scaler.fit_transform(X)
```

### Estruturas de Dados Eficientes
- **KD-Tree:** Para baixas dimens√µes
- **Ball Tree:** Para altas dimens√µes
- **LSH (Locality Sensitive Hashing):** Para datasets muito grandes

## Casos de Uso

- **Sistemas de recomenda√ß√£o:** "Usu√°rios similares a voc√™ tamb√©m compraram..."
- **Reconhecimento de padr√µes:** Classifica√ß√£o de imagens
- **An√°lise de sentimentos:** Classifica√ß√£o de textos
- **Detec√ß√£o de anomalias:** Identificar pontos muito distantes dos vizinhos
- **Preenchimento de valores faltantes:** Usar m√©dia dos K vizinhos

## Quando N√ÉO usar KNN

- **Datasets muito grandes:** Performance inadequada
- **Muitas dimens√µes:** Maldi√ß√£o da dimensionalidade
- **Dados esparsos:** Dist√¢ncias se tornam menos significativas
- **Tempo real cr√≠tico:** Predi√ß√µes s√£o lentas

## Overfitting e Underfitting no KNN

O algoritmo KNN √© particularmente sens√≠vel aos problemas de **overfitting** e **underfitting** dependendo do valor de K escolhido:

### Overfitting (K muito pequeno)
- **K = 1:** O modelo se torna muito espec√≠fico aos dados de treino
- **Sens√≠vel a ru√≠do:** Outliers podem influenciar demais as predi√ß√µes
- **Fronteiras de decis√£o irregulares:** Muitas "ilhas" de classifica√ß√£o
- **Alta vari√¢ncia:** Pequenas mudan√ßas nos dados afetam muito o resultado

### Underfitting (K muito grande)
- **K muito alto:** O modelo se torna muito generalista
- **Perde padr√µes locais:** Ignora estruturas importantes nos dados
- **Fronteiras de decis√£o muito suaves:** Pode n√£o capturar complexidade real
- **Alto vi√©s:** Simplifica demais o problema

### Encontrando o K Ideal
```python
# Exemplo de valida√ß√£o cruzada para encontrar melhor K
from sklearn.model_selection import cross_val_score
from sklearn.neighbors import KNeighborsClassifier

k_values = range(1, 31)
cv_scores = []

for k in k_values:
    knn = KNeighborsClassifier(n_neighbors=k)
    scores = cross_val_score(knn, X_train, y_train, cv=5)
    cv_scores.append(scores.mean())

optimal_k = k_values[cv_scores.index(max(cv_scores))]
```

**üí° Para mais detalhes sobre overfitting e underfitting:** [Overfitting e Underfitting](../machine-learning/overfitting-underfitting.md)

## KNN no Contexto de Machine Learning

O K-Vizinhos √© fundamentalmente um **algoritmo de machine learning** supervisionado, usado para:
- **Sistemas de recomenda√ß√£o** (Netflix, Spotify)
- **Detec√ß√£o de anomalias** (fraudes, outliers)
- **Classifica√ß√£o de imagens** e reconhecimento de padr√µes
- **Processamento de linguagem natural**

Para uma vis√£o completa do KNN no contexto de ML, incluindo pipelines de produ√ß√£o, compara√ß√µes com deep learning e aplica√ß√µes pr√°ticas, veja: **[K-Vizinhos em Machine Learning](../machine-learning/k-vizinhos-ml.md)**
